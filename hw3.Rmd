---
title: "Social Data Science Homework 3: Natural language processing"
author: "Your name here"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
# Do not edit this chunk

# The following lines define how the output of code chunks should behave
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(include = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(error = TRUE)

# Required packages, please install any you do not have
library(rmarkdown)
library(tidyverse)
library(knitr)
library(stringr)
library(tidytext)
library(word2vec)
library(stm)
library(ggplot2)
library(viridis)
library(parallel)
library(reshape2)
library(magrittr)
```

# Instructions

This assignment is designed to build your familiarity with the natural language processing techniques covered in class. As in the previous assignments, it will involve a combination of short written answers and coding in R. All answers should be written in this document. *Please write answers to written questions outside of the code cells rather than as comments.*

### Requirements
You should be viewing this document in RStudio. If you have not done so already, make sure to install the required packages (see initial chunk). You can do this by clicking the ``Install`` button in the Packages tab in the lower-right corner of RStudio and following the directions on the installation menu. You can also install packages by entering ``install.packages(x)`` into the R Console, where ``x`` is the name of the package.

### Submitting the homework
Once you have finished the assignment please complete the following steps to submit it:

1. Click on the ``Knit`` menu at the top of the screen and select ``Knit to HTML``. This will execute the all of the code and render the RMarkdown document in HTML. Verify that this document contains all of your answers and that none of the chunks produce error messages.
2. Add this document *and* the HTML file to Github. Use ``Homework submitted`` as your main commit message.
3. Push the commit to Github.
4. Visit the Github repository in your browser and verify that the final version of both files has been correctly uploaded.

# **Part I: From text to vector representations**

The data consist of a set of tweets from 12 prominent politicians in the United States, 6 Democrats and 6 Republicans. The entire timeline for each politician was collected (Twitter provides the ~3200 most recent tweets) but the dataset has been filtered to contain tweets from 2020 onwards. Note that politicians who tweet very frequently might not have any tweets for 2020 (i.e. if they wrote more than 3200 tweets in 2021).

*Please make sure to run the chunk above to load the necessary packages for this assignment. You may need to install some of the packages if you have not done so already* 

Once you have done this, run this chunk to load the data. The regular expressions should remove hashtags, mentions, and URLs, as well as exact duplicates.
```{r loading data, echo=FALSE, tidy=TRUE, eval=TRUE, include=FALSE}
data <- read_csv('data/politics_twitter.csv')
data <- data %>% 
    mutate(text = gsub("#[A-Za-z0-9]+|@[A-Za-z0-9]", "", text)) %>% # Removing hashtags and mentions
    mutate(text = gsub("(http[^ ]*)|(www.[^ ]*)", "", text)) %>% # Removing URLs
    distinct(text, .keep_all =TRUE) # Removing duplicates
print(unique(data$screen_name)) # This shows the screen names
```

### Downsampling data (Optional)
I encourage you to attempt this assignment using the full version of the dataset. If you find that your computer is struggling due to the size of the data (crashing, overheating, running out of memory, etc.), then uncomment and run the cell below to take a random sample of the data. You may change `n` to be smaller or larger as necessary, but we warned that very small samples may render much of the following analysis meaningless. You may also sub-sample later on for specific parts of the assignment, just make sure to comment your code to make it clear if you are doing this.

*Even if you plan to use the full dataset, I would recommend using a sub-sample for testing your code. Once you have answered all questions you can go back and run the code with all the data.*
```{r sampling, echo=FALSE, tidy=TRUE, eval=TRUE, include=FALSE}
#n <- 5000
#data <- sample_n(data, n)
```

### Questions
Q.1 Before analyzing the language, let's take a look at the dataset to see what it contains. Please write code to do the following.

a. Use group_by and summarize to print the names of the politicians and the number of tweets they each wrote.

b. Use group_by and summarize to print the median number of tweets per politician for each month and year in the dataset. The results should show a single value for each month-year pair.
```{r q1, echo=FALSE, tidy=TRUE}
# a

# b
```

Q.2: Complete the arguments for `unnest_tokens` to count all of the words in the corpus. Answer the question below.
```{r q2, echo=TRUE, tidy=TRUE}
words <- data %>% unnest_tokens()

words %>% count(word, sort = TRUE) %>%
  slice(1:10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) + geom_col() +
  labs(y = NULL, x='Term frequency', title=paste("10 most frequent terms in corpus"))
```
What are the top three most frequent words? Explain why this result is expected based on Zipf's law.

Q.3: Let's remove the stopwords. If you run the code below, you will see that the term `amp` is the most frequent term. Add this term to the `stop_words` list. Hint: You will need to create an object with the same structure as `stop_words` then merge them together. You can add any string in the lexicon column as it is ignored in the join.
```{r q3, echo=TRUE, tidy=TRUE}
data(stop_words) 
stop_words <- stop_words %>% filter(lexicon == "snowball") # specifying snowball stopword lexicon
stop_words <- # Add "amp" to stopwords

words.2 <- words %>% anti_join(stop_words)

words.2 %>% count(word, sort = TRUE) %>%
  slice(1:10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) + geom_col() +
  labs(y = NULL, x='Term frequency', title=paste("10 most frequent terms"), caption = "Stopwords removed.")
```

Q4. Let's analyze how the language used by each politician varies and how this varies over time. Complete the `group_by` statement to count the words used by each politician in each year. Next, assign `X` to be the name of one of the people in the dataset. Answer the question below.
```{r q4, echo=TRUE, tidy=TRUE}
person.year.counts <- data %>% unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(word, screen_name, year)

top_terms <- 
  person.year.counts %>%
  group_by(screen_name, year) %>% # TODO: Complete
  top_n(10, n) %>%
  ungroup() %>%
  arrange(word, -n)

X <- "" # Choose the screen name of a politician
top_terms  %>% filter(screen_name == X) %>% # Remove filter
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = factor(year))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ year, scales = "free") + scale_fill_viridis_d() +
  coord_flip() + labs(y="Word frequency", x="Term", title = paste("Top 5 words used by ", X,   " in 2020-2021"))
```
What do the results tell you about this politician? Does their language vary between 2020 and 2021?

Q5. Modify the filter to look at another politician. Answer the questions below.
```{r q5, echo=TRUE, tidy=TRUE}
Y <- "" # Choose another figure here
top_terms %>% filter(screen_name == Y) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = factor(year))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ year, scales = "free") + scale_fill_viridis_d() +
  coord_flip() + labs(y="Word frequency", x="Term", title = paste("Top 5 words used by", Y, "in each year"))
```
Do you notice any differences between the two politicians?

Q6. Let's create a TF-IDF weighted document-term matrix, where each document is a politician-year pair. This will help to compare how politicians use language. Add arguments to the `unite` function to create a new column called `person_year` where the value is the string for the person's handle and the year, e.g "AOC_2021". Make sure to include an argument to avoid dropping the original columns.
```{r q6, echo=TRUE, tidy=TRUE}
# Let's add a column with the total frequency of each word
word.totals <- person.year.counts %>% 
  group_by(word) %>% 
  summarize(total = sum(n))

person.year <- left_join(person.year.counts, word.totals) %>%
  unite() # add arguments here

tfidf <- person.year %>%  filter(total >= 10) %>% 
  bind_tf_idf(word, person_year, n)
```

Q7. By weighting terms by their TF-IDF scores, where a document is treated as all the lines by a politician in a given year, we can better distinguish the language unique to particular politicians. Run this chunk and answer the questions below.
```{r q7, echo=TRUE, tidy=TRUE}
top_tfidf <- 
  tfidf %>%
  group_by(screen_name, year) %>%
  top_n(5, tf_idf) %>%
  ungroup() %>%
  arrange(word, -tf_idf)

top_tfidf %>% filter(screen_name == X) %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(word, tf_idf, fill = factor(year))) +
  geom_col(show.legend = FALSE) + 
  facet_wrap(~ year, scales = "free") + scale_fill_viridis_d() +
  coord_flip() + labs(y="TF-IDF score", x="Term", title = paste("Top 5 words used by", X, "in each year "))

top_tfidf %>% filter(screen_name == Y) %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(word, tf_idf, fill = factor(year))) +
  geom_col(show.legend = FALSE) + 
  facet_wrap(~ year, scales = "free") + scale_fill_viridis_d() +
  coord_flip() + labs(y="TF-IDF score", x="Term", title = paste("Top 5 words used by", Y, "in each year"))
```
How do the results vary now that we have used TF-IDF weighting? What do the results tell you about these two politicians?

We can use this data to construct a TF-IDF weighted document-term matrix (DTM).
```{r dtm, echo=TRUE, tidy=TRUE}
### Do not modify this cell
M <- tfidf %>%
  cast_dtm(person_year, word, tf_idf) %>% as.matrix()

for (i in 1:dim(M)[1]) { # Normalizing every column
  M[i,] <- M[i,]/sqrt(sum(M[i,]^2))
}

sims <- M %*% t(M)
```

Q8. Using the similarity matrix, find the 10 most similar politician-year pairs. Print each pair and the similarity score. Make sure to ignore any entries on the diagonal.
```{r q8, echo=TRUE, tidy=TRUE}
### Your code below
```
What do you observe when you look at the results? Do these similarities make any sense given the differences between these figures' political views?

# *Part 2: Word embeddings*

Let's continue our analysis using word embeddings.

Q9. Use the `word2vec` function to train an embedding model using the *entire corpus*. Complete the function to use the skip-gram model with 100 dimensional vectors, a window size of 3, and 5 negative samples. Set the minimum word count to equal 10.

To speed things up we will set `threads = detectCores()` to run the process in parallel.
```{r q9, echo=TRUE, tidy=TRUE}
set.seed(10980)

print(detectCores()) # This will show how many cores you have available
model <- word2vec::word2vec(x = tolower(data$text), 
                  type=, # complete argument
                  dim=, # complete argument
                  window = , # complete argument
                  negative= , # complete argument
                  iter=10L,
                  threads = detectCores())
```

Let's analyze how politicians are represented in this model. Complete the function to select a vector representing President Biden (you will need to ensure the term you use is in the vocabulary, you will get an error if a word is not recognized). Answer the question below.
```{r q9, echo=TRUE, tidy=TRUE}
keyword <- "" # add appropriate term here
predict(model, keyword, type = "nearest", top_n = 10)
```
What do you notice about the results? 

Q.10. We can use this embedding model to construct representations of each politician's language use. Use the `doc2vec` function to embed all of the tweets each person wrote in 2021. The results should contain a single embedding for each tweet. The `doc2vec` function requires that the data be in a particular format (see `newdata` argument in documentation). Modify the code below to create this new column and include it in the selected columns, then provide the data in the appropriate format to the `doc2vec` function.
```{r q10, echo=TRUE, tidy=TRUE}
to_embed <- data %>% mutate() %>% select(, text, screen_name) # mutate to create column required by doc2vec, make sure to add it to the select statement
embs <- doc2vec(model, ) %>% as_data_frame # add data in appropriate format to the doc2vec function
```

Q.11. Next, we want to take the average over all the tweet embeddings for each politician. Missing rows are dropped because some tweets have invalid embeddings (no words in vocabulary). Complete the `summarize_if` argument to take the mean for all numeric columns.

The result should contain a row vector for each politician.
```{r q11, echo=TRUE, tidy=TRUE}
embs$screen_name <- to_embed$screen_name # adding screen name back in
politician.embeddings <- embs %>% drop_na() %>% group_by(screen_name) %>% summarize_if() # complete arguments
print(dim(politician.embeddings)) # should be 12, 101
```

Q.12. Now we can extract these politician vectors and compute the cosine similartity between them.

We can use a heatmap to visualize the similarities between politicians. To do this we use the `melt` function from the `reshape2` package to transform the similarity matrix into a dataframe where each cell is now represented as a separate row, along with the names of each politician in the pair. 

The diagonal values are set to be missing, otherwise they interfere with the shading of the color palette. 
```{r q12, echo=TRUE, tidy=TRUE}
### Do not modify any code here (if it doesn't work, adjust code above)
### Answer questions below
M2 <- politician.embeddings %>% select_if(is.numeric) %>% as.matrix()

for (i in 1:dim(M2)[1]) { # Normalizing every column
  M2[i,] <- M2[i,]/sqrt(sum(M2[i,]^2))
}

sims2 <- M2 %*% t(M2) # computing cosine similarity matrix
colnames(sims2) <- politician.embeddings$screen_name
rownames(sims2) <- colnames(sims2)
diag(sims2) <- NA

melted <- melt(sims2)
colnames(melted) <- c("i", "j", "sim")

ggplot(melted, aes(x=i, y=j, fill=sim)) + 
  geom_tile() + scale_fill_viridis_c() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```
Do these results give you any insight into the politicians? Which politicians appear to be similar to each other? What are some of the most dissimilar pairs?

# *Part 3: Topic models*

For the final part of this assignment you will train a topic model on the corpus and analyze the results. We will use a structural topic model with prevalence and content covariates.

The code below creates a new variable called `party`, indicating whether a politician is a Democrat or a Republican. To help make the computation easier we will restrict our focus to the subset of tweets that were written in 2021.

Q.13
Modify the function to use the same set of stopwords as used in the analysis above. You will also need to modify the `removestopwords` argument otherwise the model will remove the stopwords above and those in the preset lexicon.
```{r q13, echo=TRUE, tidy=TRUE}
data$party <- factor(ifelse(data$screen_name %in% c("JoeBiden","KamalaHarris","SpeakerPelosi","BernieSanders","AOC","SenSchumer"), "Democrat", "Republican"))
data <- data %>% filter(year == 2021)
meta <- data %>% select(party, screen_name, month)
processed.docs <- textProcessor(data$text, metadata = meta) # Add the two stopwords arguments
output <- prepDocuments(processed.docs$documents, processed.docs$vocab, processed.docs$meta, lower.thresh = 10)
```

Q.14. Complete the `stm` function to run an initial topic modeling. Pick a reasonable value for `K` and modify the relevance argument to consist of a non-linear function of the year. Add arguments to allow prevalence to vary as a function of month and party and content to vary as a function of party.

*This code may take up to 5 minutes to run. I recommend testing it using a sub-sample of the data before running it for the entire dataset.*
```{r q14, echo=TRUE, tidy=TRUE}
K <- # set k
fit <- stm(documents = output$documents, vocab = output$vocab, 
           K=K,
           data = output$meta, 
           prevalence = , # Add prevalence formula
           content = , # Add content formula
           verbose = TRUE
           )
```

Q.15. We can plot the topic proportions to get the topics that occur most frequently. 

We can extract these values by manipulating results of the `make.dt` function, which provides us with a vector of topic proportions for each document. This code uses `doc.props` and `doc.count` create a object creating the average proportion of each topic over all documents (the result should have the dimension `K x 1`). The results the top 5 topics with the highest proportions in the corpus. 

Add arguments to `summarize_if` to take the sum of all numeric columns in `doc.props`.
```{r q15, echo=TRUE, tidy=TRUE}
plot(fit, type = "summary")

doc.props <- make.dt(fit) # gets document proportions
doc.count <- dim(doc.props)[1] # gets number of documents
top5<-  doc.props %>% summarize_if() %>% # complete arguments
  select_if(str_detect(names(.), "Topic")) %>%
  divide_by(doc.count) %>% t() %>% as.data.frame() %>%
  top_n(5)
print(top5)
```

Q.16. Explore these five topics using any of the functions covered in lecture or in the `stm` documentation (e.g. `findThoughts`, `labelTopics`) then provide names and descriptions below.
```{r q16, echo=TRUE, tidy=TRUE}

```
Name and describe each of the five topics.

  1. Name: Description:
  2. Name: Description:
  3. Name: Description:
  4. Name: Description:
  5. Name: Description:
  
Q. 17. Use the`estimateEffect` function covered in lecture to analyze the relationship between the covariate and the topic. 

Modify the first argument of the function to accept the same formula as used above.

Next, modify the `topics` argument of `plot` to select the five topics covered above and change the `custom.labels` argument to contain the names you assigned to topics in the previous question.
```{r q17, echo=TRUE, tidy=TRUE}
prep <- estimateEffect(, fit, meta = output$meta) # add formula
plot(prep, "month", method = "continuous", topics = c(), model = fit, xaxt = "n", xlab = "month",  # complete topics argument
     labeltype = "custom", custom.labels = c()) # complete names argument
```
What do you notice? Are these topics stable over time or do they vary over time? Are these changes meaningful in the context of current events?

Q.18 Select a topic of interest and plot the differences in content by partisanship. Answer the question below.
```{r q18, echo=TRUE, tidy=TRUE}
plot(fit, type="perspectives", topics=) # Add topic number
```
What is this topic about? Do the differences appear to be meaningful?

This is the end of the assignment. Follow the submission instructions at the beginning of this document. The procedure is the same as for the previous assignments, but please read the note on line 48 carefully.